{
  "hash": "fde744a44ac5e553eb8b78d68143a5f9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Day 27\"\ntitle-slide-attributes:\n  data-background-image: images/lake.jpg\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\nsubtitle: \"Math 216: Statistical Thinking\"\nauthor: \"Bastola\"\nformat:\n  revealjs:\n    html-math-method: mathjax\n    mathjax-url: \"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/MathJax.js\" \n    theme: [default, scss/main.scss]\n    slide-number: true\n    preview-links: auto\n    history: true\n    chalkboard: \n      src: drawings.json\n    transition: slide\n    background-transition: fade    \n    touch: false\n    controls: true\n---\n\n\n\n\n## Recap {.font70}\n\n::: row\n::: left\n\n```{mermaid}\n%%| echo: false\ngraph TD\n  A[Start] --> B{\"σ known?\"}\n  B -->|Yes| C[\"Use z-test/z-interval\"]\n  B -->|No| D{\"n ≥ 30?\"}\n  D -->|Yes| E[\"CLT: Use t-test (z ≈ t)\"]\n  D -->|No| F[\"Normal? QQ-plot/test\"]\n  F -->|Yes| G[Use t-test]\n  F -->|No| H[Non-parametric test]\n```\n\n:::\n\n::: right\n\n> With small samples (n < 30), normality checks become critical. Let’s examine real data from the `Davis` dataset (car package) of self-reported vs actual weights:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\ndata(Davis)\nsmall_sample <- Davis$weight[1:15]  # Small subsample\nad.test(small_sample)$p.value \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.911712e-06\n```\n\n\n:::\n:::\n\n:::\n:::\n\n## Challenges with Non-normal Distributions {.font70}\n\n::: cle9\n\n**What if the population data is decidedly non-normal?**\n\n- **Small Sample Sizes and Non-normality**: When sample sizes are small ($n < 30$) and the data is non-normal, traditional tests like t-tests may become unreliable. This can lead to inflated **Type I errors**—incorrectly rejecting the null hypothesis ($H_0$) when it is true.\n\n- **Nonparametric Statistics**: These tests do not assume a normal distribution. Instead, they rely on ranks or medians, making them robust to outliers and extreme values.\n:::\n\n\n## Visual Diagnostics: The Illusion of Normality (QQ plot) {.font70}\n\n**Example**: 15-weight sample from `Davis` dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\nqqPlot(small_sample, main=\"QQ-Plot: Small Sample\") + theme_tufte()\n```\n\n::: {.cell-output-display}\n![](Day27_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n:::\n\n\n## Visual Diagnostics: The Illusion of Normality (Histogram) {.font70}\n\n**Example**: 20-weight sample from `Davis` dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tibble(x=small_sample), aes(x)) + \n  geom_histogram(fill=\"#1f77b4\", bins=5) + \n  geom_vline(xintercept=57, color=\"red\") +\n  labs(title=\"Seemingly Normal? (n=15)\")\n```\n\n::: {.cell-output-display}\n![](Day27_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n## Case Study 1: Davis Weight Data (n=15) {.font80}\n\n::: cle8\n**Population Context**: Full dataset (N=200) has median=57kg, but our sample (first 15 obs) has median=68kg:\n\n::: {.cell}\n\n```{.r .cell-code}\nSIGN.test(small_sample, md=57)$p.value  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03515625\n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(small_sample, mu=57)$p.value     \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05803929\n```\n\n\n:::\n:::\n\n**Resolution**: Sign test detects true median shift (68 vs 57) while t-test is confused by:\n\n- Right skew (γ₁ = 1.2)\n- Outlier (166kg) inflating mean (64.1 vs median 68)\n:::\n\n## Case Study 2: Simulated Skewed Data (n=15) {.font80}\n\n**Population**: Lognormal distribution (median=7.38, mean=12.18)\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nskewed_pop <- exp(rnorm(1000, mean=2))  # True median=7.38\nsamp <- sample(skewed_pop, 15)\n\n# Wrong approach: t-test for median\nt.test(samp, mu=7.38)$p.value    \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.626605\n```\n\n\n:::\n\n```{.r .cell-code}\n# Right approach: Sign test\nSIGN.test(samp, md=7.38)$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6072388\n```\n\n\n:::\n:::\n\n\n## Type I Error Rates (10,000 Simulations) {.font80}\n\n**When H₀ is TRUE** (testing median=7.38 in lognormal population):\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(456)\nerr_rates <- replicate(10000, {\n  samp <- sample(skewed_pop, 15)\n  c(\n    t = t.test(samp, mu = 7.38)$p.value < 0.05,\n    sign = SIGN.test(samp, md = 7.38)$p.value < 0.05\n  )\n})\n\n# Get one error rate per method:\nrowMeans(err_rates)  \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     t   sign \n0.0956 0.0354 \n```\n\n\n:::\n:::\n\n**Results**:\n\n1. T-test falsely rejects 9.6% of time (inflated Type I error)\n2. Sign test maintains 3.5% error rate\n\n\n##  Recommendations {.font80}\n\n::: cle6\n1. **Small n**: Use sign test unless strong evidence of normality\n2. **Visual Cues**:\n   - Always pair histograms (≤5 bins) with QQ-plots\n   - Treat \"normal-looking\" plots with skepticism\n3. **Test Alignment**:\n   - Means → t-test (requires normality)\n   - Medians → sign test (requires only ranked data)\n:::\n\n## How P-values are Calculated: Sign Test {.font80}\n\n**Binomial Foundation**: Under $H_0$: median $= \\eta_0$, each observation has 50% chance of being above/below $\\eta_0$\n\n**Davis Example** ($H_0$: $\\eta = 57$ kg):\n\n::: cle8\n::: {.cell}\n\n```{.r .cell-code}\nsmall_sample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  77  58  53  68  59  76  76  69  71  65  70 166  51  64  52\n```\n\n\n:::\n\n```{.r .cell-code}\nabove <- sum(small_sample > 57)  \nabove\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n\n```{.r .cell-code}\nn <- length(small_sample - 57)\nn\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 15\n```\n\n\n:::\n:::\n\n:::\n\n## **Exact Binomial Formula**: {.font60}\n\n$$\n\\begin{aligned}\n\\text{p-value} &= 2 \\times P(X \\geq 12) \\\\\n&= 2 \\times \\sum_{k=12}^{15} \\binom{15}{k} (0.5)^{15} \\\\\n&= 2 \\times (0.01389 + 0.00320 + 0.00046 + 0.00003) \\\\\n&= 0.03516\n\\end{aligned}\n$$\n\n**R Calculation**:\n\n::: {.cell}\n\n```{.r .cell-code}\n2 * pbinom(11, 15, 0.5, lower.tail=FALSE)  # Matches SIGN.test()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.03515625\n```\n\n\n:::\n:::\n\n```r\nSIGN.test(small_sample, md=57)\n    One-sample Sign-Test\n\ndata:  small_sample\ns = 12, p-value = 0.03516\nalternative hypothesis: true median is not equal to 57\n95 percent confidence interval:\n 58.17817 75.10916\n\n```\n\n",
    "supporting": [
      "Day27_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}